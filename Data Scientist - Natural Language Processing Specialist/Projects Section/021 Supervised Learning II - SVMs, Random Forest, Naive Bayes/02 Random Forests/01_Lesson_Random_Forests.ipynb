{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11028294",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eb4a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac15144",
   "metadata": {},
   "source": [
    "## 2. Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547a033",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- We’ve written some code to print the number of rows in the data set and the distribution of safety ratings of the entire dataset. \n",
    "- Uncomment the relevant lines (under `## 1.`)\n",
    "- Are the safety classes distributed equally in the dataset?\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 2**  \n",
    "- Now that we know the safety variable classes are equally distributed, we’re going to create a bootstrapped sample using `.sample()`.\n",
    "- `.sample()` takes two arguments:\n",
    "    - number of rows: which is the same size as the dataset\n",
    "    - `replace`: set to `True` because bootstrapping is sampling with replacement.\n",
    "- Uncomment the lines under `## 2.` and then print the distribution of safety ratings of the new sampled dataset.\n",
    "- What is the safety ratings distribution of the bootstrapped data?\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 3**  \n",
    "- Note that the distribution has now shifted! \n",
    "- Using the same process, write a for loop to create 1000 bootstrapped samples of the same size as the original dataset. \n",
    "- Save the percentage of “low” ratings into an array called `low_perc`.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 4**  \n",
    "- We’ve written some code to plot a histogram of the low percentage values. \n",
    "- Uncomment the lines under `## 4.`\n",
    "- We see that the average value of the low safety proportion of vehicles spans a range centered around the true mean.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 5**  \n",
    "- Now uncomment the lines under `## 5.` to print the average low percentage and the 95% confidence range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252ddc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1728\n",
      "Distribution of safety ratings in 1728 of data:\n",
      "safety\n",
      "low     0.333333\n",
      "med     0.333333\n",
      "high    0.333333\n",
      "Name: proportion, dtype: float64\n",
      "Distribution of safety ratings in bootstrapped sample data:\n",
      "safety\n",
      "med     0.350116\n",
      "low     0.332176\n",
      "high    0.317708\n",
      "Name: proportion, dtype: float64\n",
      "0.33279861111111114\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average low percentage: 0.3328\n",
      "95% Confidence Interval for low percengage: (0.3096,0.3536)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "nrows = df.shape[0]\n",
    "\n",
    "## 1. Print number of rows and distribution of safety ratings\n",
    "print(nrows)\n",
    "print(f'Distribution of safety ratings in {nrows} of data:')\n",
    "print(df.safety.value_counts(normalize=True))\n",
    "\n",
    "## 2. Create bootstrapped sample\n",
    "boot_sample = df.sample(nrows, replace=True)\n",
    "print(f'Distribution of safety ratings in bootstrapped sample data:')\n",
    "print(boot_sample.safety.value_counts(normalize=True))\n",
    "\n",
    "\n",
    "## 3. Create 1000 bootstrapped samples\n",
    "low_perc = []\n",
    "for i in range(1000):\n",
    "    boot_sample = df.sample(nrows, replace=True)\n",
    "    low_perc.append(boot_sample.safety.value_counts(normalize=True)['low'])\n",
    "\n",
    "\n",
    "## 4. Plot a histogram of the low percentage values\n",
    "mean_lp = np.mean(low_perc) \n",
    "print(mean_lp)\n",
    "plt.hist(low_perc, bins=20)\n",
    "plt.xlabel('Low Percentage')\n",
    "plt.show()\n",
    "\n",
    "## 5. What are the 2.5 and 97.5 percentiles?\n",
    "print(f'Average low percentage: {np.mean(low_perc).round(4)}')\n",
    "\n",
    "low_perc.sort()\n",
    "print(f'95% Confidence Interval for low percengage: ({low_perc[25].round(4)},{low_perc[975].round(4)})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf97524",
   "metadata": {},
   "source": [
    "## 3. Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd9a138",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- Train a decision tree with `max_depth` set to 5. \n",
    "- Evaluate the `accuracy_score` on the `test` data.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 2**  \n",
    "- We’ve written some code to get a new set of indices, `ids`, to generate a bootstrapped set of row indices. \n",
    "- We’ve set the `random_state` argument to `0` for reproducibility. \n",
    "- Using these indices, fit another decision tree to training data pertaining to these rows. \n",
    "- What is the accuracy score on the test set for the new classifier?\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 3**  \n",
    "- Repeat a decision tree build on 10 different bootstrapped samples using a for loop. \n",
    "- Save the results, `y_pred` of all 10 predictions on the test set in an array, `preds`. \n",
    "- Take the average of the 10 results and save it as `ba_pred`\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 4**  \n",
    "- We have just performed bagging! Calculate the accuracy score on the bagged predictions and save it as `ba_accuracy`. \n",
    "- (Note that the predictions are averaged and will no longer be binary as a bunch of zeroes and ones have been averaged.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "837e2a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of DT on test set (trained using full set): 0.8588\n",
      "Accuracy score of DT on test set (trained using bootstrapped sample): 0.8912\n",
      "Accuracy score of aggregated 10 bootstrapped samples:0.9097\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "#1.Decision tree trained on  training set\n",
    "dt = DecisionTreeClassifier(max_depth=5)\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "print(f'Accuracy score of DT on test set (trained using full set): {np.round(dt.score(x_test, y_test), 4)}')\n",
    "\n",
    "#2. New decision tree trained on bootstrapped sample\n",
    "dt2 = DecisionTreeClassifier(max_depth=5)\n",
    "#ids are the indices of the bootstrapped sample\n",
    "ids = x_train.sample(x_train.shape[0], replace=True, random_state=0).index\n",
    "dt2.fit(x_train.loc[ids], y_train.loc[ids])\n",
    "print(f'Accuracy score of DT on test set (trained using bootstrapped sample): {np.round(dt2.score(x_test, y_test), 4)}')\n",
    "\n",
    "## 3. Bootstapping ten samples and aggregating the results:\n",
    "preds = []\n",
    "random_state = 0\n",
    "#Write for loop:\n",
    "for i in range(10):\n",
    "    ids = x_train.sample(x_train.shape[0], replace=True, random_state=random_state+i).index\n",
    "    dt = DecisionTreeClassifier(max_depth=5)\n",
    "    dt.fit(x_train.loc[ids], y_train.loc[ids])\n",
    "    y_pred = dt.predict(x_test)\n",
    "    preds.append(y_pred)\n",
    "\n",
    "ba_pred = np.array(preds).mean(0)\n",
    "\n",
    "# 4. Calculate accuracy of the bagged sample\n",
    "ba_accuracy = accuracy_score(ba_pred>=0.5, y_test)\n",
    "print(f'Accuracy score of aggregated 10 bootstrapped samples:{np.round(ba_accuracy, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8685ab5",
   "metadata": {},
   "source": [
    "## 4. Random Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da8b2e",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- We’ve written some code that create a random sample of 10 features, `rand_features` from the 15 original features by using NumPy’s `random.choice` method and defined a decision tree classifier `dt2`.\n",
    "- Train the new decision tree model (without specifying any pre-selected parameters!), `dt2` on training data that *contains only these ten columns*. \n",
    "- Calculate the new accuracy on the test set, store it as `accuracy_dt2` and print it to compare it to the tree built using the entire training set.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 2**  \n",
    "- Every time we use `np.random.choice`, we can generate a new subset of random features. \n",
    "- We’re now going to obtain predictions from ten decision tree classifiers by doing the following. \n",
    "- We’ve initialized an empty array called `predictions` and created a `for` loop with ten iterations with the following steps:\n",
    "    - The first line selects ten random features using `rand_features = np.random.choice(x_train.columns, 10)`\n",
    "    - Fit the decision tree `dt2` to training data pertaining to these features.\n",
    "    - Append the predictions on test data (with only `rand_features` columns!) to the array `predictions`\n",
    "- Uncomment the relevant lines.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 3**  \n",
    "- We have ten decision trees’ worth of predictions now! \n",
    "- To meaningfully combine their predictions, let’s use the following system. \n",
    "- If more than 5 classifiers predict that a datapoint belong to a certain class, we assign an aggregate prediction to that class. \n",
    "- To do this:\n",
    "    - Create an array `prob_predictions` that is the mean value of the predictions stored in `predictions`. This is similar to a probability on each datapoint since this is a binary classification problem.\n",
    "    - Create an array agg_predictions that assigns a value of False or True to each element in `prob_predictions` based on whether the value is <= 0.5 or >0.5. This gives us the *aggregate* prediction from the 10 decision tree classifiers.\n",
    "    - Calculate the accuaracy score on this aggregate prediction array. Store it as `agg_accuracy` and print it to compare it to the previous accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87edb988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of DT on test set (trained using full feature set):\n",
      "0.9467592592592593\n",
      "Accuracy score of DT on test set (trained using random feature sample):\n",
      "0.6782407407407407\n",
      "Accuracy score of aggregated 10 samples:\n",
      "0.7731481481481481\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train, y_train)\n",
    "print(\"Accuracy score of DT on test set (trained using full feature set):\")\n",
    "accuracy_dt = dt.score(x_test, y_test)\n",
    "print(accuracy_dt)\n",
    "\n",
    "# 1. Create rand_features, random samples from the set of features\n",
    "rand_features = np.random.choice(x_train.columns, 10)\n",
    "\n",
    "# Make new decision tree trained on random sample of 10 features and calculate the new accuracy score\n",
    "dt2 = DecisionTreeClassifier()\n",
    "dt2.fit(x_train[rand_features], y_train)\n",
    "accuracy_dt2 = dt2.score(x_test[rand_features], y_test)\n",
    "\n",
    "print(\"Accuracy score of DT on test set (trained using random feature sample):\")\n",
    "print(accuracy_dt2)\n",
    "\n",
    "# 2. Build decision trees on 10 different random samples \n",
    "predictions = []\n",
    "for i in range(10):\n",
    "    rand_features = np.random.choice(x_train.columns,10)\n",
    "    dt2.fit(x_train[rand_features], y_train)\n",
    "    predictions.append(dt2.predict(x_test[rand_features]))\n",
    "\n",
    "## 3. Get aggregate predictions and accuracy score\n",
    "prob_predictions = np.array(predictions).mean(0)\n",
    "agg_predictions = (prob_predictions > 0.5)\n",
    "agg_accuracy = accuracy_score(agg_predictions, y_test)\n",
    "print('Accuracy score of aggregated 10 samples:')\n",
    "print(agg_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359bcc8d",
   "metadata": {},
   "source": [
    "## 5. Bagging in `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2bbc0a",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- Create an instance of `BaggingClassifier`, `bag_dt` with the following arguments:\n",
    "    - `DecisionTreeClassifier` (with `max_depth=5)` base estimator\n",
    "    - `n_estimators=10`.\n",
    "- Fit the model on the training set and evaluate the model on the test set by calculating the accuracy score. \n",
    "- Save the score as `bag_accuracy` and print it.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 2**  \n",
    "- Create a different bagging classifier, `bag_dt_10` that includes the parameter `max_features=10` over and above the parameters in `bag_dt`. \n",
    "- Fit the model on the training set and evaluate the model on the test set by calculating the accuracy score. \n",
    "- Save the score as `bag_accuracy_10` and print it.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 3**  \n",
    "- Change the base estimator of the bagged classifier to be logistic regression and call this instance `bag_lr` as follows:\n",
    "    - Set base_estimator to be `LogisticRegression`()\n",
    "    - Set `n_estimators` and `max_features` to be 10 like in the previous checkpoint\n",
    "- Refit on the training set and calculate the accuracy on the test set. \n",
    "- Store this as `bag_accuracy_lr` and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c493e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of Bagged Classifier, 10 estimators:\n",
      "0.9143518518518519\n",
      "Accuracy score of Bagged Classifier, 10 estimators, 10 max features:\n",
      "0.8125\n",
      "Accuracy score of Logistic Regression, 10 estimators, 10 max features:\n",
      "0.9050925925925926\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "# 1. Bagging classifier with 10 Decision Tree base estimators\n",
    "bag_dt = BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=5), n_estimators=10)\n",
    "bag_dt.fit(x_train, y_train)\n",
    "bag_accuracy = bag_dt.score(x_test, y_test)\n",
    "\n",
    "print('Accuracy score of Bagged Classifier, 10 estimators:')\n",
    "print(bag_accuracy)\n",
    "\n",
    "# 2.Set `max_features` to 10.\n",
    "bag_dt_10 = BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=5), n_estimators=10, max_features=10)\n",
    "bag_dt_10.fit(x_train, y_train)\n",
    "bag_accuracy_10 = bag_dt_10.score(x_test, y_test)\n",
    "print('Accuracy score of Bagged Classifier, 10 estimators, 10 max features:')\n",
    "print(bag_accuracy_10)\n",
    "\n",
    "\n",
    "# 3. Change base estimator to Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "bag_lr = BaggingClassifier(estimator=LogisticRegression(), n_estimators=10, max_features=10)\n",
    "bag_lr.fit(x_train, y_train)\n",
    "bag_accuracy_lr = bag_lr.score(x_test, y_test)\n",
    "\n",
    "print('Accuracy score of Logistic Regression, 10 estimators, 10 max features:')\n",
    "print(bag_accuracy_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add1eab7",
   "metadata": {},
   "source": [
    "## 6. Train and Predict using `scikit-learn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36bb669",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- Create a random forest classification model defined as `rf` with default parameters. \n",
    "- Use the `.get_params()` method to get the parameters of the random forest. \n",
    "- Store it as `rf_params` and print it.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 2**  \n",
    "- Fit `rf` using the training data set and labels. \n",
    "- Predict the classes of the test data set (`x_test`) and save this as an array `y_pred`. \n",
    "- Calculate the accuracy of model on the test set (either using `.score()` or `accuracy_score()`) and save it as `rf_accuracy`.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 3**  \n",
    "- Implement additional classification evaluation metrics — calculate and print the precision, recall, and confusion matrix on the test set. \n",
    "- Store them as `rf_precision`, `rf_recall` and `rf_confusion_matrix` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ab84948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "Test set accuracy:\n",
      "0.9513888888888888\n",
      "Test set precision: 0.9457364341085271\n",
      "Test set recall: 0.8970588235294118\n",
      "Test set confusion matrix:\n",
      "[[289   7]\n",
      " [ 14 122]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "y = df['accep']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "# 1. Create a Random Forest Classifier and print its parameters\n",
    "rf = RandomForestClassifier()\n",
    "rf_params = rf.get_params()\n",
    "print('Random Forest parameters:')\n",
    "print(rf_params)\n",
    "\n",
    "# 2. Fit the Random Forest Classifier to training data and calculate accuracy score on the test data\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "print('Test set accuracy:')\n",
    "rf_accuracy = rf.score(x_test, y_test)\n",
    "print(rf_accuracy)\n",
    "\n",
    "# 3. Calculate Precision and Recall scores and the Confusion Matrix\n",
    "rf_precision = precision_score(y_test, y_pred)\n",
    "print(f'Test set precision: {rf_precision}')\n",
    "rf_recall = recall_score(y_test, y_pred)\n",
    "print(f'Test set recall: {rf_recall}')\n",
    "rf_confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(f'Test set confusion matrix:\\n{rf_confusion_matrix}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b653c8",
   "metadata": {},
   "source": [
    "## 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a72c80b",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- Initialize and fit a `RandomForestRegressor()` model named `rfr` on the training data. \n",
    "- Calculate the default scores (the R^2 values here) on the train and test sets, store them as `r_squared_train` and `r_squared_test` respectively, and print them.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 2**  \n",
    "- Calculate the average price of a car, store it as `avg_price`. \n",
    "- Calculate the MAE (Mean Absolute Error) for the train and test sets and store the values as `mae_train` and `mae_test`. \n",
    "- Print all three values to to see how the errors compare to the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce5411c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count     1728.000000\n",
      "mean     36476.712880\n",
      "std      13457.464931\n",
      "min       4786.538756\n",
      "25%      25779.729816\n",
      "50%      36383.240521\n",
      "75%      47528.160340\n",
      "max      67246.782150\n",
      "Name: price, dtype: float64\n",
      "Train set R^2: 0.9772821150013223\n",
      "Test set R^2: 0.8339109800951005\n",
      "Avg Price Train/Test: 36476.7128804898\n",
      "Train set MAE: 1595.3337323474311\n",
      "Test set MAE: 4480.9920316610405\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data', names=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'accep'])\n",
    "df['accep'] = ~(df['accep']=='unacc') #1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:,0:6], drop_first=True)\n",
    "\n",
    "## Generating some fake prices for regression! :) \n",
    "fake_prices = (15000 + 25*df.index.values)+np.random.normal(size=df.shape[0])*5000\n",
    "df['price'] = fake_prices\n",
    "print(df.price.describe())\n",
    "y = df['price']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state=0, test_size=0.25)\n",
    "\n",
    "# 1. Create a Random Regressor and print `R^2` scores on training and test data\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(x_train, y_train)\n",
    "r_squared_train = rfr.score(x_train, y_train)\n",
    "r_squared_test = rfr.score(x_test, y_test)\n",
    "\n",
    "print(f'Train set R^2: {r_squared_train}')\n",
    "print(f'Test set R^2: {r_squared_test}')\n",
    "\n",
    "# 2. Print Mean Absolute Error on training and test data\n",
    "y_pred_train = rfr.predict(x_train)\n",
    "y_pred_test = rfr.predict(x_test)\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "avg_price = y.mean()\n",
    "\n",
    "print(f'Avg Price Train/Test: {avg_price}')\n",
    "print(f'Train set MAE: {mae_train}')\n",
    "print(f'Test set MAE: {mae_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b0af2",
   "metadata": {},
   "source": [
    "## 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43f65b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec065ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fc3f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy-XphA9WxU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
