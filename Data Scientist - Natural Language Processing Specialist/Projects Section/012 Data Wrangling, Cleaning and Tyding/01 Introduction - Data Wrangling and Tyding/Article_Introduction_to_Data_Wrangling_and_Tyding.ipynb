{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Wrangling and Tidying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Frequently when we work with data, we encounter unstructured and/or *messy data*. \n",
    "- Messy data can take a variety of forms. \n",
    "- This could mean any of the following:\n",
    "    - The columns are mislabeled or do not have variable names.\n",
    "    - The dataset contains nonsensical data.\n",
    "    - Variables are stored in both the columns and rows.\n",
    "- While the data may be messy, it is still extremely informative. \n",
    "- We need to clean, transform, and sometimes manipulate the data structure to gain any insights. \n",
    "- This process is often called *data wrangling* or *data munging*.\n",
    "- At the final stages of the data wrangling process, we will have a dataset that we can easily use for modeling purposes or for visualization purposes. \n",
    "- This is a *tidy dataset* where each column is a variable and each row is an observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let’s look at a subset of restaurant inspections from the [New York City Department of Health and Mental Hygiene](https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j) (NYC DOHMH) to work through some data wrangling processes. \n",
    "- The data includes seven different columns with information about a restaurant’s location and health inspection. \n",
    "- Here is a description of the dataset’s variables.\n",
    "\n",
    "\n",
    "| Pos. | Var. Name           | Var. Description                    |\n",
    "|------|---------------------|-------------------------------------|\n",
    "| 0    | DBA                 | Restaurant name                     |\n",
    "| 1    | BORO                | Borough                             |\n",
    "| 2    | CUISINE DESCRIPTION | Type of cuisine                     |\n",
    "| 3    | GRADE               | Letter Grade                        |\n",
    "| 4    | LATITUDE            | Latitude coordinates of restaurant  |\n",
    "| 5    | LONGITUDE           | Longitude coordinates of restaurant |\n",
    "| 6    | URL                 | URL link to restaurant's website    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let’s use the `read_csv()` function in pandas to load our dataset as a pandas dataframe and take a look at the first 10 rows out of the 27 total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "restaurants = pd.read_csv(\"DOHMH_restaurant_inspections.csv\")\n",
    "\n",
    "# the .head(10) function will show us the first 10 rows in our dataset\n",
    "print(restaurants.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Images/dscp-data-wrangling-restaurant.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `.shape` method in pandas identifies the number of rows and columns in our dataset as (rows, columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(27, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When we look closely at the table, we see some missing data. \n",
    "- In both `GRADE` and `URL` columns, we have missing values marked as NaNs. \n",
    "- In the `Latitude` and `Longitude` columns, we have a missing set of coordinates marked as (0.000, 0.000) for IHOP. \n",
    "- (0.000, 0.000) is the label for missing coordinates because no restaurants in New York City are at the equator. \n",
    "- Other common indicators used for missing values are values that are NA and -."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are also duplicate rows for the restaurant labeled Seamore’s. To remove any duplicate rows, we can use the `drop_duplicates()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the .drop_duplicates() function removes duplicate rows\n",
    "restaurants = restaurants.drop_duplicates() \n",
    "\n",
    "# the .head(10) function will show us the first 10 rows in our dataset\n",
    "print(restaurants.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Images/dscp-data-wrangling-resaurants-dropduplic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `.shape` method again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(25, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After dropping duplicate rows, we are left with 25 rows and can now see “YOLANDA RESTAURANT” at the bottom when looking at our dataset’s first 10 rows.\n",
    "\n",
    "<br>\n",
    "\n",
    "- If we look at the first four columns of our data: `DBA`, `BORO`, `CUISINE DESCRIPTION`, and `GRADE`. \n",
    "- These column names are all capitalized, while the columns following it use both uppercase and lowercase. \n",
    "- To have some consistency across column names, we will iterate over the column names of our dataset and convert them all to lowercase using the `map()` and `lower()` functions.\n",
    "- We also need to make sure to include the `str` function to identify that we are working with strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map() applies the str.lower() function to each of the columns in our dataset to convert the column names to all lowercase\n",
    "restaurants.columns = map(str.lower, restaurants.columns)\n",
    "\n",
    "# the .head(10) function will show us the first 10 rows in our dataset\n",
    "print(restaurants.head(10))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Images/dscp-data-wrangling-restaurants.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may have noticed that the first column of the dataset is called `DBA`, but we know it is a column with restaurant names. \n",
    "- We can use the `rename()` function and a dictionary to relabel our columns. \n",
    "- While we are renaming our columns, we might also want to shorten the `cuisine description` column to just `cuisine`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis=1` refers to the columns, `axis=0` would refer to the rows\n",
    "# In the dictionary the key refers to the original column name and the value refers to the new column name {'oldname1': 'newname1', 'oldname2': 'newname2'}\n",
    "restaurants = restaurants.rename({'dba': 'name', 'cuisine description': 'cuisine'}, axis=1)\n",
    "\n",
    "\n",
    "# the .head(10) function will show us the first 10 rows in our dataset\n",
    "print(restaurants.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Images/dscp-data-wrangling-restaurants(1).webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Great! Now we have a dataset that is cleaner and much easier to read!. \n",
    "- Let’s take a look at each column’s data types by appending `.dtypes` to our pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column Name          | Data Type |\n",
    "|----------------------|-----------|\n",
    "| name                 | object    |\n",
    "| boro                 | object    |\n",
    "| cuisine              | object    |\n",
    "| grade                | object    |\n",
    "| latitude             | float64   |\n",
    "| longitude            | float64   |\n",
    "| url                  | object    |\n",
    "| dtype: object        |           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have two types of variables: object and float64.\n",
    "- `object` can consist of both strings or mixed types (both numeric and non-numeric), and `float64` are numbers with a floating point (ie. numbers with decimals). \n",
    "- There are other data types such as `int64` (integer numbers), `bool` (True/False values), and `datetime64` (date and/or time values).\n",
    "\n",
    "<br>\n",
    "\n",
    "- Since we have both continuous (float64) and categorical (object) variables in our data, it might be informative to look at the number of unique values in each column using the `nunique()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .nunique() counts the number of unique values in each column \n",
    "restaurants.nunique() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column Name          | Unique Values |\n",
    "|----------------------|---------------|\n",
    "| name                 | 25            |\n",
    "| boro                 | 4             |\n",
    "| cuisine              | 15            |\n",
    "| grade                | 1             |\n",
    "| latitude             | 24            |\n",
    "| longitude            | 24            |\n",
    "| url                  | 16            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see that our data consists of 4 boroughs in New York and 15 cuisine types. \n",
    "- We know that we also have missing data in `url` from our initial inspection of the data, so the unique number of values in `url` might not be super informative.\n",
    "- Additionally, we have corrected for duplicate restaurants, so the restaurant name, latitude, longitude, and url should be unique to each restaurant (unless, of course, there are some restaurant chains at different locations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From our initial inspection of the data, we know we have missing data in `grade`, `url`, `latitude`, and `longitude`. \n",
    "- Let’s take a look at how the data is missing, also referred to as missingness. \n",
    "- To do this we can use `isna()` to identify if the value is missing. \n",
    "- This will give us a boolean and indicate if the observation in that column is missing (True) or not (False). \n",
    "- We will also use `sum()` to count the number of missing values, where `isna()` returns True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts the number of missing values in each column \n",
    "restaurants.isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column Name          | Missing Values |\n",
    "|----------------------|----------------|\n",
    "| name                 | 0              |\n",
    "| boro                 | 0              |\n",
    "| cuisine              | 0              |\n",
    "| grade                | 15             |\n",
    "| latitude             | 0              |\n",
    "| longitude            | 0              |\n",
    "| url                  | 9              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see that there are missing values in the grade and url columns, but no missing values in `latitude` and `longitude`. \n",
    "- However, coordinates at (0.000, 0.000) are not valid for any of the restaurants in our dataset, and we saw that these exist in our initial analysis. \n",
    "- To handle this, we will replace the (0.000, 0.000) coordinates with NaN values.\n",
    "\n",
    "<br>\n",
    "\n",
    "- We will use the `.where()` method from the pandas library to replace the invalid coordinates. \n",
    "- The `.where()` method keeps the values specified in its first argument, and replaces all other values with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here our .where() function replaces latitude values less than 40 with NaN values\n",
    "restaurants['latitude'] = restaurants['latitude'].where(restaurants['latitude'] > 40) \n",
    "\n",
    "# here our .where() function replaces longitude values greater than -70 with NaN values\n",
    "restaurants['longitude'] = restaurants['longitude'].where(restaurants['longitude'] < -70) \n",
    "\n",
    "# .sum() counts the number of missing values in each column\n",
    "restaurants.isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column Name          | Missing Values |\n",
    "|----------------------|----------------|\n",
    "| name                 | 0              |\n",
    "| boro                 | 0              |\n",
    "| cuisine              | 0              |\n",
    "| grade                | 15             |\n",
    "| longitude            | 2              |\n",
    "| latitude             | 2              |\n",
    "| url                  | 9              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we see that latitude and longitude each have two missing data points thanks to replacing the (0.000, 0.000) values with NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterizing missingness with crosstab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let’s try to understand the missingness in the url column by counting the missing values across each borough. \n",
    "- We will use the `crosstab()` function in pandas to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(\n",
    "    # tabulates the boroughs as the index\n",
    "    restaurants['boro'],  \n",
    "        \n",
    "    # tabulates the number of missing values in the url column as columns\n",
    "    restaurants['url'].isna(), \n",
    "    \n",
    "    # names the rows\n",
    "    rownames = ['boro'],\n",
    "    \n",
    "    # names the columns \n",
    "    colnames = ['url is na']\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| url is na | False | True  |\n",
    "|-----------|-------|-------|\n",
    "| boro      |       |       |\n",
    "| BRONX     | 1     | 1     |\n",
    "| BROOKLYN  | 2     | 4     |\n",
    "| MANHATTAN | 11    | 2     |\n",
    "| QUEENS    | 2     | 2     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We see that most of the restaurants in Manhattan in our dataset have restaurant links, while most restaurants in Brooklyn do not have url links."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It might be easier to read what url links are by removing the prefixes of the websites, such as “https://www.”. \n",
    "- We will use `str.lstrip()` to remove the prefixes. \n",
    "- Similar to when we were working with our column names, we need to make sure to include the `str` function to identify that we are working with strings and `lstrip` to remove parts of the string from the left side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .str.lstrip('https://') removes the “https://” from the left side of the string\n",
    "restaurants['url'] = restaurants['url'].str.lstrip('https://') \n",
    "\n",
    "# .str.lstrip('www.') removes the “www.” from the left side of the string\n",
    "restaurants['url'] = restaurants['url'].str.lstrip('www.') \n",
    "\n",
    "# the .head(10) function will show us the first 10 rows in our dataset\n",
    "print(restaurants.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Images/dscp-data-wrangling-restaurants-url-clea.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amazing! Our dataset is now much easier to read and use. \n",
    "- We have identifiable columns and variables that are easy to work with thanks to our data wrangling process. \n",
    "- We also corrected illogical data values and made the strings a little easier to read.\n",
    "\n",
    "<br>\n",
    "\n",
    "- In this example, we worked with data that was rather tidy, in the sense that each row was an observation (a restaurant) and each column was a variable. \n",
    "- However, what if our dataset was not tidy? \n",
    "- What if our columns and rows needed reorganization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let’s take a look at a dataset that has information about the average annual wage for restaurant workers across New York City boroughs and New York City as a whole from the years 2000 and 2007. \n",
    "- The data is from the [New York State Department of Labor](https://dol.ny.gov/labor-data), Quarterly Census of Employment and Wages,and only contains six total rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_wage = pd.read_csv(\"annual_wage_restaurant_boro.csv\")\n",
    "print(annual_wage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Images/dscp-data-wrangling-annual-wage-boro-data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are three variables in this dataset: `borough`, `year`, and `average annual income`. \n",
    "- However, we have values (2000 and 2007) in the column headers rather than variable names (`year` and `average annual income`). \n",
    "- This is not ideal to work with, so let’s fix it! \n",
    "- We will use the `melt()` function in pandas to turn the current values (2000 and 2007) in the column headers into row values and add `year` and `avg_annual_wage` as our column labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_wage=annual_wage.melt(\n",
    "    # which column to use as identifier variables\n",
    "    id_vars=[\"boro\"], \n",
    "    \n",
    "    # column name to use for “variable” names/column headers (ie. 2000 and 2007) \n",
    "    ar_name=[\"year\"], \n",
    "    \n",
    "    # column name for the values originally in the columns 2000 and 2007\n",
    "    value_name=\"avg_annual_wage\"\n",
    ") \n",
    "\n",
    "print(annual_wage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](Images/dscp-data-wrangling-annual-wage-boro-long.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have a tidy dataset where each column is a variable (borough, year, or average annual wage), and each row is an observation! This dataset will be much easier to work with moving forward!\n",
    "- You now have the tools to continue to gather more information about New York City restaurants to answer questions you are interested in exploring. \n",
    "- For example, we could explore what variables are most predictive of receiving a passing health score. \n",
    "- We would need to gather some more data and go through the data wrangling process to end up with tidy data that is ready for analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy-XphA9WxU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
