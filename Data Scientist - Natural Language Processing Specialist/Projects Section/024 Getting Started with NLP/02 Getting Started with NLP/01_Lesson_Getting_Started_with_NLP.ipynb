{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "477d29cd",
   "metadata": {},
   "source": [
    "# Getting Started With Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f3cf8",
   "metadata": {},
   "source": [
    "## 2. Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63116c1f",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- We used NLTK’s `PorterStemmer` to normalize the text — run the code to see how it does.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 2**  \n",
    "- In the output terminal you’ll see our program counts `\"go\"` and `\"went\"` as different words! \n",
    "- Also, what’s up with `\"mani\"` and `\"hardli\"`? \n",
    "- A lemmatizer will fix this. Let’s do it.\n",
    "- Where `lemmatizer` is defined, replace None with `WordNetLemmatizer()`.\n",
    "- Where we defined `lemmatized`, replace the empty list with a list comprehension that uses `lemmatizer` to `lemmatize()` each `token` in `tokenized`.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 3**  \n",
    "- Why are the lemmatized verbs like `\"went\"` still conjugated? \n",
    "- By default `lemmatize()` treats every word as a noun.\n",
    "- Give `lemmatize()` a second argument: \n",
    "    - `get_part_of_speech(token)`. \n",
    "- This will tell our lemmatizer what part of speech the word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7660a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['so', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'i', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'i', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "Lemmatized text:\n",
      "['So', 'many', 'squid', 'be', 'jump', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'pack', 'valise', 'I', 'go', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arrive', 'She', 'hardly', 'even', 'notice']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from helper_functions import get_part_of_speech\n",
    "\n",
    "\n",
    "text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text)  # \\W+ - matches non-word characters\n",
    "tokenized = word_tokenize(cleaned)  # split text into words/tokens\n",
    "\n",
    "# Stemming and Lemmatization\n",
    "# Example word: are\n",
    "#    Stemming = are\n",
    "#    Lemmatization = be\n",
    "\n",
    "# Stemming is mostly just removing the suffix or prefixes from a word\n",
    "# Example: The stem of the word 'running' is 'run' or 'are' is 'are'\n",
    "# Stemming is faster than lemmatization\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "# Lemmatization is the process of converting a word to its base form\n",
    "# Example: The lemma of the word 'running' is 'run'\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = list(map(lambda x: lemmatizer.lemmatize(x, get_part_of_speech(x)), tokenized))\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d81af",
   "metadata": {},
   "source": [
    "## 3. Parsing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58080008",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- Run the code to see the silly squid sentences parsed into dependency trees visually!\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 2**  \n",
    "- Change `my_sentence` to a sentence of your choosing and run the code again to see it parsed out as a tree!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "390258b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So many squids are jumping out of suitcases these days.\n",
      "        jumping                \n",
      "  _________|________________    \n",
      " |   |   squids    out      |  \n",
      " |   |     |        |       |   \n",
      " |   |    many      of     days\n",
      " |   |     |        |       |   \n",
      "are  .     So   suitcases these\n",
      "\n",
      "You can barely go anywhere without seeing one.\n",
      "          go                       \n",
      "  ________|____________________     \n",
      " |   |    |       |      |  without\n",
      " |   |    |       |      |     |    \n",
      " |   |    |       |      |   seeing\n",
      " |   |    |       |      |     |    \n",
      "You can barely anywhere  .    one  \n",
      "\n",
      "I went to the dentist the other day.\n",
      "          went               \n",
      "  _________|_________         \n",
      " |   |     to        |       \n",
      " |   |     |         |        \n",
      " |   |  dentist     day      \n",
      " |   |     |      ___|____    \n",
      " I   .    the   the     other\n",
      "\n",
      "Sure enough, I saw an angry one jump out of my dentist's bag.\n",
      "                   saw                           \n",
      "  __________________|_________                    \n",
      " |   |   |    |              jump                \n",
      " |   |   |    |      _________|__________         \n",
      " |   |   |    |     |    |    |         out      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |    |     |    |    |          of      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |    |     |    |    |         bag      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |   Sure   |    |    |       dentist    \n",
      " |   |   |    |     |    |    |     _____|_____   \n",
      " ,   I   .  enough  an angry one   my          's\n",
      "\n",
      "She hardly even noticed.\n",
      "    noticed         \n",
      "  _____|__________   \n",
      "She  hardly even  . \n",
      "\n",
      "How does spaCy know how to parse this sentence?\n",
      "               know                   \n",
      "  ______________|_________             \n",
      " |   |     |    |       parse         \n",
      " |   |     |    |     ____|______      \n",
      " |   |     |    |    |    |   sentence\n",
      " |   |     |    |    |    |      |     \n",
      "How does spaCy  ?   how   to    this  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "\n",
    "squids_text = \"So many squids are jumping out of suitcases these days. You can barely go anywhere without seeing one. I went to the dentist the other day. Sure enough, I saw an angry one jump out of my dentist's bag. She hardly even noticed.\"\n",
    "\n",
    "# Load the 'en_core_web_sm' model\n",
    "# More info can be found here: https://spacy.io/models/en#en_core_web_sm\n",
    "# This model contains word vectors, tokenization, part-of-speech tagging, named entity recognition, dependency parser\n",
    "dependency_parser = spacy.load('en_core_web_sm')\n",
    "parsed_squids = dependency_parser(squids_text)\n",
    "\n",
    "my_sentence = \"How does spaCy know how to parse this sentence?\"\n",
    "my_parsed_sentence = dependency_parser(my_sentence)\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        parsed_child_nodes = [to_nltk_tree(child) for child in node.children]\n",
    "        return Tree(node.orth_, parsed_child_nodes)\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "for sent in parsed_squids.sents:\n",
    "    print(sent)\n",
    "    to_nltk_tree(sent.root).pretty_print()\n",
    "\n",
    "for sent in my_parsed_sentence.sents:\n",
    "    print(sent)\n",
    "    to_nltk_tree(sent.root).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b6ae2c",
   "metadata": {},
   "source": [
    "## 4. Language Models: Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e647e29e",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- We’ve turned a passage from *Through the Looking Glass* by Lewis Carroll into a list of words (aside from stopwords, which we’ve removed) using `nltk` preprocessing.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 2**  \n",
    "- Now let’s turn this list into a bag-of-words using `Counter()`!\n",
    "- Comment out the print statement and set `bag_of_looking_glass_words` equal to a call of `Counter()` on `normalized`. \n",
    "- Print `bag_of_looking_glass_words`. \n",
    "- What are the most common words?\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 3**  \n",
    "- Try changing `text` to another string of your choosing and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e27915e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['excellent', 'bag', 'word', 'excellent', 'word', 'bag']\n",
      "Counter({'excellent': 2, 'bag': 2, 'word': 2})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from texts import looking_glass_text\n",
    "from helper_functions import get_part_of_speech\n",
    "\n",
    "\n",
    "text = looking_glass_text\n",
    "text = \"Such an excellent bag of words and an excellent word 'bags'.\"\n",
    "\n",
    "# Clean and tokenize text\n",
    "cleaned = re.sub('\\W+', ' ', text).lower()  # \\W+ - matches non-word characters\n",
    "tokenized = word_tokenize(cleaned) \n",
    "\n",
    "# Remove stop words\n",
    "# Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning\n",
    "stop_words = stopwords.words('english')\n",
    "filtered = list(filter(lambda x: x not in stop_words, tokenized))\n",
    "\n",
    "# Lemmatize the tokens\n",
    "# Lemmatization is the process of converting a word to its base form\n",
    "# Example: The lemma of the word 'running' is 'run'\n",
    "normalizer = WordNetLemmatizer()\n",
    "normalized = list(map(lambda x: normalizer.lemmatize(x, get_part_of_speech(x)), filtered))\n",
    "print(normalized)\n",
    "\n",
    "# Define bag_of_looking_glass_words & print:\n",
    "bag_of_looking_glass_words = Counter(normalized)\n",
    "print(bag_of_looking_glass_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8702a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text (Removing non-word characters):\n",
      "such an excellent bag of words and an excellent word bags \n",
      "\n",
      "Tokenized text (Splitting text into tokens):\n",
      "['such', 'an', 'excellent', 'bag', 'of', 'words', 'and', 'an', 'excellent', 'word', 'bags']\n",
      "\n",
      "Filtered text (Removing stop words):\n",
      "['excellent', 'bag', 'words', 'excellent', 'word', 'bags']\n",
      "\n",
      "Normalized text (Lemmatizing the tokens):\n",
      "['excellent', 'bag', 'word', 'excellent', 'word', 'bag']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaned text (Removing non-word characters):\")\n",
    "print(cleaned, end=\"\\n\\n\")\n",
    "print(\"Tokenized text (Splitting text into tokens):\")\n",
    "print(tokenized, end=\"\\n\\n\")\n",
    "print(\"Filtered text (Removing stop words):\")\n",
    "print(filtered, end=\"\\n\\n\")\n",
    "print(\"Normalized text (Lemmatizing the tokens):\")\n",
    "print(normalized, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23562279",
   "metadata": {},
   "source": [
    "## 5. Language Models: N-Gram and NLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725b2185",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- If you run the code, you’ll see the 10 most commonly used words in Through the Looking Glass parsed with NLTK’s `ngrams` module — if you’re thinking this looks like a bag of words, that’s because it is one!\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 2**  \n",
    "- What do you think the most common phrases in the text are? \n",
    "- Let’s find out…\n",
    "- Where `looking_glass_bigrams` is defined, change the second argument to `2` to see bigrams. \n",
    "- Change `n` to `3` for `looking_glass_trigrams` to see trigrams.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 3**  \n",
    "- Change `n` to a number greater than `3` for `looking_glass_ngrams`. \n",
    "- Try increasing the number.\n",
    "- At what `n` are you just getting lines from poems repeated in the text? \n",
    "- This is where there may be too few examples of each sequence within your training corpus to make any helpful predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f020bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking Glass Bigrams:\n",
      "[(('of', 'the'), 101), (('said', 'the'), 98), (('in', 'a'), 97), (('in', 'the'), 90), (('as', 'she'), 82), (('you', 'know'), 72), (('a', 'little'), 68), (('the', 'queen'), 67), (('said', 'alice'), 67), (('to', 'the'), 66)]\n",
      "\n",
      "Looking Glass Trigrams:\n",
      "[(('the', 'red', 'queen'), 54), (('the', 'white', 'queen'), 31), (('said', 'in', 'a'), 21), (('she', 'went', 'on'), 18), (('said', 'the', 'red'), 17), (('thought', 'to', 'herself'), 16), (('the', 'queen', 'said'), 16), (('said', 'to', 'herself'), 14), (('said', 'humpty', 'dumpty'), 14), (('the', 'knight', 'said'), 14)]\n",
      "\n",
      "Looking Glass 7 n-grams:\n",
      "[(('one', 'and', 'one', 'and', 'one', 'and', 'one'), 7), (('and', 'one', 'and', 'one', 'and', 'one', 'and'), 6), (('twas', 'brillig', 'and', 'the', 'slithy', 'toves', 'did'), 3), (('brillig', 'and', 'the', 'slithy', 'toves', 'did', 'gyre'), 3), (('and', 'the', 'slithy', 'toves', 'did', 'gyre', 'and'), 3), (('the', 'slithy', 'toves', 'did', 'gyre', 'and', 'gimble'), 3), (('slithy', 'toves', 'did', 'gyre', 'and', 'gimble', 'in'), 3), (('toves', 'did', 'gyre', 'and', 'gimble', 'in', 'the'), 3), (('did', 'gyre', 'and', 'gimble', 'in', 'the', 'wabe'), 3), (('gyre', 'and', 'gimble', 'in', 'the', 'wabe', 'all'), 3)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from texts import looking_glass_full_text\n",
    "\n",
    "\n",
    "# Clean and tokenize text\n",
    "cleaned = re.sub('\\W+', ' ', looking_glass_full_text).lower()   # \\W+ - matches non-word characters\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# Change the n value to 2:\n",
    "looking_glass_bigrams = ngrams(tokenized, 2)\n",
    "looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)\n",
    "\n",
    "# Change the n value to 3:\n",
    "looking_glass_trigrams = ngrams(tokenized, 3)\n",
    "looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)\n",
    "\n",
    "# Change the n value to a number greater than 3:\n",
    "n = 7\n",
    "looking_glass_ngrams = ngrams(tokenized, n)\n",
    "looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)\n",
    "\n",
    "print(\"Looking Glass Bigrams:\")\n",
    "print(looking_glass_bigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass Trigrams:\")\n",
    "print(looking_glass_trigrams_frequency.most_common(10))\n",
    "\n",
    "print(f\"\\nLooking Glass {n} n-grams:\")\n",
    "print(looking_glass_ngrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a1cb9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('the', 'cat', 'is'): 1,\n",
       "         ('cat', 'is', 'asleep'): 1,\n",
       "         ('is', 'asleep', 'the'): 1,\n",
       "         ('asleep', 'the', 'cat'): 1,\n",
       "         ('the', 'cat', 'purrs'): 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The cat is asleep. The cat purrs.\"\n",
    "cleaned = re.sub('\\W+', ' ', sentence).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "Counter(ngrams(tokenized, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d2834",
   "metadata": {},
   "source": [
    "## 6. Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cb50a1",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- Check out how the bag of words model and tf-idf models stack up when faced with a new Sherlock Holmes text!\n",
    "- Run the code as is to see what topics they uncover…\n",
    "\n",
    "<br>\n",
    "\n",
    "**Task 2**  \n",
    "- Tf-idf has some interesting findings, but the regular bag of words is full of words that tell us very little about the topic of the texts!\n",
    "- Let’s fix this. \n",
    "- Add some words to `stop_list` that don’t tell you much about the topic and then run your code again. \n",
    "- Do this until you have at least 10 words in `stop_list` so that the bag of words LDA model has some interesting topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bd2ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Topics found by bag of words LDA ~~~\n",
      "Topic #1: mccarthy find father hand\n",
      "Topic #2: find street cry sit\n",
      "Topic #3: majesty understand touch eye\n",
      "Topic #4: majesty king photograph sherlock\n",
      "Topic #5: cry mccarthy turner right\n",
      "Topic #6: find father case part\n",
      "Topic #7: son case mccarthy young\n",
      "Topic #8: street find time two\n",
      "Topic #9: find mr call make\n",
      "Topic #10: give leave mccarthy back\n",
      "\n",
      "\n",
      "~~~ Topics found by tf-idf LDA ~~~\n",
      "Topic #1: client investigation lens utter\n",
      "Topic #2: say holmes one upon\n",
      "Topic #3: mccarthy say father holmes\n",
      "Topic #4: form resolution ruin retain\n",
      "Topic #5: complete stand disappear bristol\n",
      "Topic #6: king majesty holmes photograph\n",
      "Topic #7: harness depend whatever factor\n",
      "Topic #8: overpower crime moment recognise\n",
      "Topic #9: eye pronounce couple easy\n",
      "Topic #10: agent whether felt james\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from texts import bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3\n",
    "from helper_functions import preprocess_text\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# preparing the text\n",
    "corpus = [bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3]\n",
    "preprocessed_corpus = [preprocess_text(chapter, lemmatizer, stop_words) for chapter in corpus]\n",
    "\n",
    "\n",
    "# Update stop_list:\n",
    "stop_list = [\"say\", \"see\", \"holmes\", \"shall\", \"say\", \"man\", \"upon\", \"know\", \"quite\", \"one\", \"well\", \"could\", \"would\", \"take\", \"may\", \"think\", \"come\", \"go\", \"little\", \"must\", \"look\"]\n",
    "\n",
    "# filtering topics for stop words\n",
    "def filter_out_stop_words(corpus):\n",
    "    no_stops_corpus = []\n",
    "    for chapter in corpus:\n",
    "        no_stops_chapter = \" \".join([word for word in chapter.split(\" \") if word not in stop_list])\n",
    "        no_stops_corpus.append(no_stops_chapter)\n",
    "    return no_stops_corpus\n",
    "filtered_for_stops = filter_out_stop_words(preprocessed_corpus)\n",
    "\n",
    "# creating the bag of words model\n",
    "bag_of_words_creator = CountVectorizer()\n",
    "bag_of_words = bag_of_words_creator.fit_transform(filtered_for_stops)\n",
    "\n",
    "# creating the tf-idf model\n",
    "tfidf_creator = TfidfVectorizer(min_df = 0.2)\n",
    "tfidf = tfidf_creator.fit_transform(preprocessed_corpus)\n",
    "\n",
    "# creating the bag of words LDA model\n",
    "lda_bag_of_words_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_bag_of_words = lda_bag_of_words_creator.fit_transform(bag_of_words)\n",
    "\n",
    "# creating the tf-idf LDA model\n",
    "lda_tfidf_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_tfidf = lda_tfidf_creator.fit_transform(tfidf)\n",
    "\n",
    "print(\"~~~ Topics found by bag of words LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda_bag_of_words_creator.components_):\n",
    "    message = \"Topic #{}: \".format(topic_id + 1)\n",
    "    message += \" \".join([bag_of_words_creator.get_feature_names_out()[i] for i in topic.argsort()[:-5 :-1]])\n",
    "    print(message)\n",
    "\n",
    "print(\"\\n\\n~~~ Topics found by tf-idf LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda_tfidf_creator.components_):\n",
    "    message = \"Topic #{}: \".format(topic_id + 1)\n",
    "    message += \" \".join([tfidf_creator.get_feature_names_out()[i] for i in topic.argsort()[:-5 :-1]])\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e538a639",
   "metadata": {},
   "source": [
    "## 7. Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae2f075",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- Assign the variable `three_away_from_code` a word with a Levenshtein distance of `3` from “code”.\n",
    "- Assign `two_away_from_chunk` a word with a Levenshtein distance of `2` from “chunk”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d39a4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Levenshtein distance from 'fart' to 'target' is 3!\n",
      "The Levenshtein distance from 'code' to 'order' is 3!\n",
      "The Levenshtein distance from 'chunk' to 'chunker' is 2!\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import edit_distance\n",
    "\n",
    "\n",
    "def print_levenshtein(string1, string2):\n",
    "    print(\"The Levenshtein distance from '{0}' to '{1}' is {2}!\".format(string1, string2, edit_distance(string1, string2)))\n",
    "\n",
    "# Check the distance between\n",
    "# any two words here!\n",
    "print_levenshtein(\"fart\", \"target\")\n",
    "\n",
    "# Assign passing strings here:\n",
    "three_away_from_code = \"order\"\n",
    "two_away_from_chunk = \"chunker\"\n",
    "\n",
    "print_levenshtein(\"code\", three_away_from_code)\n",
    "print_levenshtein(\"chunk\", two_away_from_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187545e5",
   "metadata": {},
   "source": [
    "## 8. Language Prediction & Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab83cad",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- Add three short stories by your favorite author or the lyrics to three songs by your favorite artist to `document1.py`, `document2.py`, and `document3.py`. \n",
    "- Then run to see a short example of text prediction.\n",
    "- Does it look like something by your favorite author or artist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731e1c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i find the memories baby it s make up the love sparks will fly they ignite our bones and all the world we light when they strike we ll testify our hearts are you left your love i tried to save us what is over that s beyond us let\n"
     ]
    }
   ],
   "source": [
    "import re, random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, deque\n",
    "from documents import training_doc1, training_doc2, training_doc3\n",
    "\n",
    "\n",
    "class MarkovChain:\n",
    "    def __init__(self):\n",
    "        self.lookup_dict = defaultdict(list)\n",
    "        self._seeded = False\n",
    "        self.__seed_me()\n",
    "    \n",
    "    def __seed_me(self, rand_seed=None):\n",
    "        if self._seeded is not True:\n",
    "            try:\n",
    "                if rand_seed is not None:\n",
    "                    random.seed(rand_seed)\n",
    "                else:\n",
    "                    random.seed()\n",
    "                self._seeded = True\n",
    "            except NotImplementedError:\n",
    "                self._seeded = False\n",
    "    \n",
    "    def add_document(self, str):\n",
    "        preprocessed_list = self._preprocess(str)\n",
    "        pairs = self.__generate_tuple_keys(preprocessed_list)\n",
    "        for pair in pairs:\n",
    "            self.lookup_dict[pair[0]].append(pair[1])\n",
    "    \n",
    "    def _preprocess(self, str):\n",
    "        cleaned = re.sub(r'\\W+', ' ', str).lower()\n",
    "        tokenized = word_tokenize(cleaned)\n",
    "        return tokenized\n",
    "    \n",
    "    def __generate_tuple_keys(self, data):\n",
    "        if len(data) < 1:\n",
    "            return\n",
    "        \n",
    "        for i in range(len(data) - 1):\n",
    "            yield [ data[i], data[i + 1] ]\n",
    "    \n",
    "    def generate_text(self, max_length=50):\n",
    "        context = deque()\n",
    "        output = []\n",
    "        if len(self.lookup_dict) > 0:\n",
    "            self.__seed_me(rand_seed=len(self.lookup_dict))\n",
    "            chain_head = [list(self.lookup_dict)[0]]\n",
    "            context.extend(chain_head)\n",
    "            \n",
    "            while len(output) < (max_length - 1):\n",
    "                next_choices = self.lookup_dict[context[-1]]\n",
    "                if len(next_choices) > 0:\n",
    "                    next_word = random.choice(next_choices)\n",
    "                    context.append(next_word)\n",
    "                    output.append(context.popleft())\n",
    "                else:\n",
    "                    break\n",
    "            output.extend(list(context))\n",
    "        return \" \".join(output)\n",
    "\n",
    "my_markov = MarkovChain()\n",
    "my_markov.add_document(training_doc1)\n",
    "my_markov.add_document(training_doc2)\n",
    "my_markov.add_document(training_doc3)\n",
    "generated_text = my_markov.generate_text()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2165c99",
   "metadata": {},
   "source": [
    "## 9. Advanced NLP Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a1dd8",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "- Assign `review` a string with a brief review of this lesson so far. \n",
    "- Next, run your code. Is the Naive Bayes Classifier accurately classifying your review?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544679ed",
   "metadata": {},
   "source": [
    "```python\n",
    "from reviews import counter, training_counts\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "\n",
    "# Add your review:\n",
    "review = \"This was a bad movie\"\n",
    "review_counts = counter.transform([review])\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "training_labels = [0] * 1000 + [1] * 1000\n",
    "\n",
    "classifier.fit(training_counts, training_labels)\n",
    "\n",
    "neg = (classifier.predict_proba(review_counts)[0][0] * 100).round()\n",
    "pos = (classifier.predict_proba(review_counts)[0][1] * 100).round()\n",
    "\n",
    "if pos > 50:\n",
    "    print(\"Thank you for your positive review!\")\n",
    "elif neg > 50:\n",
    "    print(\"We're sorry this hasn't been the best possible lesson for you! We're always looking to improve.\")\n",
    "else:\n",
    "    print(\"Naive Bayes cannot determine if this is negative or positive. Thank you or we're sorry?\")\n",
    "\n",
    "print(\"\\nAccording to our trained Naive Bayes classifier, the probability that your review was negative was {0}% and the probability it was positive was {1}%.\".format(neg, pos))\n",
    "\n",
    "\n",
    "# Output\n",
    "# We're sorry this hasn't been the best possible lesson for you! \n",
    "# We're always looking to improve.\n",
    "\n",
    "# According to our trained Naive Bayes classifier, the probability # that your review was negative was 67.0% and the probability it was \n",
    "# positive was 33.0%.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy-XphA9WxU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
