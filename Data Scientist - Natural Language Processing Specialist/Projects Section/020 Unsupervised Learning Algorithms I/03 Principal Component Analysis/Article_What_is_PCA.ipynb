{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how, when, and why to use Principal Component Analysis (PCA) as part of the Machine Learning lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the world of Machine Learning, any model that we implement will be more valuable when the features are engineered to suit the question we’re trying to answer. \n",
    "- With many datasets, we can simply include all available features, which gives us the full picture about our observations.\n",
    "-  For example, it’s straightforward to see a correlation between height and weight for a patient dataset. \n",
    "- Some datasets, however, have very large numbers of features. \n",
    "- If our example patient dataset expanded to include 20 different features, how would we visualize and correlate this data? \n",
    "- When it comes time to actually process the data and train the model, we often hit computational or complexity limits. \n",
    "- How do we leverage correlations within the data to make fewer, better features without losing the information included in the dataset?\n",
    "\n",
    "<br>\n",
    "\n",
    "- Situations like this are a great use case for implementing Principal Component Analysis. \n",
    "- PCA is a technique where we can reduce the number of features in a dataset without losing any of the information we have. \n",
    "- Sounds pretty great right? This article will cover various aspects of PCA, so let’s dive in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laying the groundwork for PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before we dive into the specifics of PCA, we need to understand the importance of information. \n",
    "- In particular, we need to understand how variance plays into the level of information in a dataset. \n",
    "- For the purposes of this article, we will be looking at a synthetic dataset about local pizza stores. \n",
    "- Let’s see what data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['revenue', 'total_customers', 'amt_flour', 'amt_tomatoes',\n",
      "       'amt_cheese'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('pizza.csv')\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each row of data pertains to an individual store, and gives information about how the store is doing overall with inventory and sales. \n",
    "- Suppose we look at just `revenue` and `total_customers`, and we see the following information:\n",
    "    | revenue | total_customers |\n",
    "    |---------|-----------------|\n",
    "    | 12345   | 500             |\n",
    "    | 13425   | 500             |\n",
    "    | 10872   | 500             |\n",
    "    | 9561    | 500             |\n",
    "- In this scenario, the value for `total_customers` has a value of 500 for every row. \n",
    "- While every row has a value, and therefore has data, this column does not provide a lot of *information*, due to the lack of variance in the values. \n",
    "- While we could include this feature in our downstream analytics, it doesn’t provide any additional value, because each row would have the same data.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Now let’s look at what the real data shows us for these two columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "revenue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_customers",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "380d174e-3340-410c-b7cf-3a62f3f3b6d3",
       "rows": [
        [
         "0",
         "2185.430534813131",
         "207"
        ],
        [
         "1",
         "4778.214378844623",
         "143"
        ],
        [
         "2",
         "3793.972738151323",
         "76"
        ],
        [
         "3",
         "3193.9631788866645",
         "188"
        ],
        [
         "4",
         "1202.0838819909643",
         "51"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>total_customers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2185.430535</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4778.214379</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3793.972738</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3193.963179</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1202.083882</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       revenue  total_customers\n",
       "0  2185.430535              207\n",
       "1  4778.214379              143\n",
       "2  3793.972738               76\n",
       "3  3193.963179              188\n",
       "4  1202.083882               51"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['revenue','total_customers']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, the real dataset has far more variance in these two columns.\n",
    "-  Since each feature has significant variance, these features provide valuable information about our observations, and should therefore be included in our analysis.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Variance alone is one indicator of the level of information in a dataset, but is not the only factor. \n",
    "- To expand on the idea of variance within a dataset, we will look at the Coefficient of Variance, or CV for short. \n",
    "- The premise here is that variance must be taken into context with the central tendencies of that dataset. \n",
    "- For example, if a dataset has a variance of 5, that will mean very different things if the mean is 2 vs. a dataset with a mean of 100.\n",
    "$$ CV = \\frac{\\text{Standard Deviation}}{\\text{Mean}} = \\frac{\\sigma}{\\mu} $$\n",
    "- In probability theory and statistics, the [**Coefficient of Variance (CV)**](https://en.wikipedia.org/wiki/Coefficient_of_variation), also known as *normalized root-mean-square deviation (NRMSD)*, **percent RMS**, and **relative standatd deviation (RSD)**, is a standardized measure of dispersion of a probability distribution or frequency distribution. \n",
    "- It is defined as the ratio if the standard deviation $\\sigma$ to the mean $\\mu$ (or its absolute value, $\\left|\\mu\\right|$), and often expressed as a percentage (\"%RSD\").\n",
    "\n",
    "<br>\n",
    "\n",
    "- Now, let’s actually calculate the Coefficient of Variance for each of our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_customers    56.054917\n",
      "revenue            48.561545\n",
      "amt_tomatoes       48.139269\n",
      "amt_cheese         46.915199\n",
      "amt_flour          46.459940\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#define function to calculate cv\n",
    "cv = lambda x: np.std(x, ddof=1) / np.mean(x) * 100\n",
    "\n",
    "print(df.apply(cv).sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All of the features in this dataset have enough variance where they will be useful in analysis. \n",
    "- Since variance is an important factor to PCA, these features will ultimately be ordered by the level of information (i.e. variance) they have.\n",
    "- For this dataset, that means, in order of importance, PCA will look at `total_customers`, `revenue`, `amt_tomatoes`, `amt_cheese` and then `amt_flour`. \n",
    "- While the results of PCA won’t resemble our original features, they will be a mathematical representation of the information contained in the original features, which has value for analytical purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The kind of information we have can vary from dataset to dataset, and thus can the Coefficient of Variance. \n",
    "- Use what you just learned on a new set of synthetic pizza store data, `pizza_new.csv`. \n",
    "- Calculate the Coefficients of Variance for each feature in the dataset. \n",
    "- Then, create a ranked order Python list for the features in the dataset in terms of information for PCA, from most important to least important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "revenue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_customers",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "amt_flour",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "amt_tomatoes",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "a7d3a7b0-ed73-4ffd-b970-a87d6b71ec8c",
       "rows": [
        [
         "0",
         "3878.4634214497514",
         "75",
         "43.81203298542354",
         "20.7646361161039"
        ],
        [
         "1",
         "1064.688335596178",
         "184",
         "19.21987135063523",
         "22.64188110590541"
        ],
        [
         "2",
         "3977.471388492856",
         "85",
         "43.34955760407533",
         "24.2356542652593"
        ],
        [
         "3",
         "1566.1999620799188",
         "123",
         "6.842027496845542",
         "9.580121721417251"
        ],
        [
         "4",
         "3548.2690844999274",
         "120",
         "7.234767115687349",
         "11.259159958061035"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>total_customers</th>\n",
       "      <th>amt_flour</th>\n",
       "      <th>amt_tomatoes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3878.463421</td>\n",
       "      <td>75</td>\n",
       "      <td>43.812033</td>\n",
       "      <td>20.764636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1064.688336</td>\n",
       "      <td>184</td>\n",
       "      <td>19.219871</td>\n",
       "      <td>22.641881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3977.471388</td>\n",
       "      <td>85</td>\n",
       "      <td>43.349558</td>\n",
       "      <td>24.235654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1566.199962</td>\n",
       "      <td>123</td>\n",
       "      <td>6.842027</td>\n",
       "      <td>9.580122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3548.269084</td>\n",
       "      <td>120</td>\n",
       "      <td>7.234767</td>\n",
       "      <td>11.259160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       revenue  total_customers  amt_flour  amt_tomatoes\n",
       "0  3878.463421               75  43.812033     20.764636\n",
       "1  1064.688336              184  19.219871     22.641881\n",
       "2  3977.471388               85  43.349558     24.235654\n",
       "3  1566.199962              123   6.842027      9.580122\n",
       "4  3548.269084              120   7.234767     11.259160"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp = pd.read_csv(\"pizza_new.csv\")\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate coefficient of variance for every feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_customers    55.682374\n",
       "amt_flour          49.214791\n",
       "revenue            48.151389\n",
       "amt_tomatoes       47.523382\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_temp = lambda x: np.std(x, ddof=1) / np.mean(x) * 100\n",
    "df_temp.apply(cv_temp).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rank order of importance from highest to lowest (in a list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['total_customers', 'amt_flour', 'revenue', 'amt_tomatoes']\n"
     ]
    }
   ],
   "source": [
    "importance_rank = []\n",
    "importance_rank = list(df_temp.apply(cv_temp).sort_values(ascending=False).index)\n",
    "print(importance_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Math Behind PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At this point, we need to address how we can actually take information from multiple features and distill it down into a smaller number of features. \n",
    "- Let’s dive deeper into each of the steps that lead to PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, we need to isolate a *Data Matrix*, another name for a dataset. \n",
    "- This data matrix holds all of the features and information that we are interested in. \n",
    "- Many datasets will have columns that hold information (i.e. features), and other columns that we want to predict (i.e. labels). \n",
    "- Using our previous pizza dataset, we have 5 features in our data matrix.\n",
    "    | revenue | total_customers | amt_flour | amt_tomatoes | amt_cheese |\n",
    "    |---------|-----------------|-----------|--------------|------------|\n",
    "    | 9931.860710 | 615.336682 | 37.662830 | 174.102712 | 139.402208 |\n",
    "    | 12397.798907 | 725.440590 | 44.424509 | 239.119556 | 168.425842 |\n",
    "    | 11983.079340 | 630.987797 | 40.259276 | 224.084121 | 146.612426 |\n",
    "    | 13910.984353 | 746.264763 | 43.633485 | 227.096619 | 170.726464 |\n",
    "    | 13083.859701 | 689.060436 | 48.964844 | 221.383478 | 154.786070 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From here, the next step of PCA is to calculate a covariance matrix. \n",
    "- Essentially, a covariance matrix is calculating how much a feature changes with changes in every other feature, i.e., we’re looking at the relative variance between any two features. \n",
    "- Mathematically, the formula for covariance between two features `X` and `Y` is:\n",
    "$$ Cov(X, Y) = \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\bar{X})(Y_i - \\bar{Y}) $$\n",
    "- We will do this equation for the relationship between each of our features, ultimately resulting in a covariance matrix that shows relationships for the entire dataset. \n",
    "- Simplifying our example dataset, we could think about our pizza dataset having five individual features with the names `a`, `b`, `c`, `d`, and `e`. \n",
    "- Our ultimate covariance matrix, thus, would end up looking like this:\n",
    "$$ \\begin{bmatrix} Cov_{a,a} & Cov_{a,b} & Cov_{a,c} & Cov_{a,d} & Cov_{a,e} \\\\ Cov_{b,a} & Cov_{b,b} & Cov_{b,c} & Cov_{b,d} & Cov_{b,e} \\\\ Cov_{c,a} & Cov_{c,b} & Cov_{c,c} & Cov_{c,d} & Cov_{c,e} \\\\ Cov_{d,a} & Cov_{d,b} & Cov_{d,c} & Cov_{d,d} & Cov_{d,e} \\\\ Cov_{e,a} & Cov_{e,b} & Cov_{e,c} & Cov_{e,d} & Cov_{e,e} \\end{bmatrix} $$\n",
    "- Luckily, with the pandas package, we can calculate a covariance matrix with the `.cov()` method. \n",
    "- For our pizza dataset, this results in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "revenue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_customers",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "amt_flour",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "amt_tomatoes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "amt_cheese",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "1fe99740-3ce1-4c37-bafb-5c0119ed5005",
       "rows": [
        [
         "revenue",
         "1752348.4078824243",
         "-3671.655564210951",
         "82.0210589996983",
         "73.33079976049108",
         "384.5005354863869"
        ],
        [
         "total_customers",
         "-3671.655564210951",
         "7342.909472185136",
         "3.61287834458916",
         "-22.544256957087953",
         "-6.924364070540846"
        ],
        [
         "amt_flour",
         "82.0210589996983",
         "3.61287834458916",
         "164.93397445921298",
         "-6.928481092691553",
         "1.9176056462812194"
        ],
        [
         "amt_tomatoes",
         "73.33079976049108",
         "-22.544256957087953",
         "-6.928481092691553",
         "62.20930290038624",
         "-0.48715455227517085"
        ],
        [
         "amt_cheese",
         "384.5005354863869",
         "-6.924364070540846",
         "1.9176056462812194",
         "-0.48715455227517085",
         "26.211397934710895"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>total_customers</th>\n",
       "      <th>amt_flour</th>\n",
       "      <th>amt_tomatoes</th>\n",
       "      <th>amt_cheese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>revenue</th>\n",
       "      <td>1.752348e+06</td>\n",
       "      <td>-3671.655564</td>\n",
       "      <td>82.021059</td>\n",
       "      <td>73.330800</td>\n",
       "      <td>384.500535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_customers</th>\n",
       "      <td>-3.671656e+03</td>\n",
       "      <td>7342.909472</td>\n",
       "      <td>3.612878</td>\n",
       "      <td>-22.544257</td>\n",
       "      <td>-6.924364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amt_flour</th>\n",
       "      <td>8.202106e+01</td>\n",
       "      <td>3.612878</td>\n",
       "      <td>164.933974</td>\n",
       "      <td>-6.928481</td>\n",
       "      <td>1.917606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amt_tomatoes</th>\n",
       "      <td>7.333080e+01</td>\n",
       "      <td>-22.544257</td>\n",
       "      <td>-6.928481</td>\n",
       "      <td>62.209303</td>\n",
       "      <td>-0.487155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amt_cheese</th>\n",
       "      <td>3.845005e+02</td>\n",
       "      <td>-6.924364</td>\n",
       "      <td>1.917606</td>\n",
       "      <td>-0.487155</td>\n",
       "      <td>26.211398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      revenue  total_customers   amt_flour  amt_tomatoes  \\\n",
       "revenue          1.752348e+06     -3671.655564   82.021059     73.330800   \n",
       "total_customers -3.671656e+03      7342.909472    3.612878    -22.544257   \n",
       "amt_flour        8.202106e+01         3.612878  164.933974     -6.928481   \n",
       "amt_tomatoes     7.333080e+01       -22.544257   -6.928481     62.209303   \n",
       "amt_cheese       3.845005e+02        -6.924364    1.917606     -0.487155   \n",
       "\n",
       "                 amt_cheese  \n",
       "revenue          384.500535  \n",
       "total_customers   -6.924364  \n",
       "amt_flour          1.917606  \n",
       "amt_tomatoes      -0.487155  \n",
       "amt_cheese        26.211398  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_var(X, Y):\n",
    "    assert len(X) == len(Y), \"X and Y must have the same number of elements\"\n",
    "    \n",
    "    diff = np.sum((X - np.mean(X)) * (Y - np.mean(Y)))\n",
    "    return diff / (len(X) - 1)\n",
    "\n",
    "\n",
    "m = np.zeros(df.columns.size**2).reshape(df.columns.size, df.columns.size)\n",
    "for i in range(df.columns.size):\n",
    "    for j in range(df.columns.size):\n",
    "        m[i, j] = calc_var(df.iloc[:, i], df.iloc[:, j])\n",
    "\n",
    "df_cov_matrix = pd.DataFrame(m, columns=df.columns, index=df.columns)\n",
    "df_cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "revenue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "total_customers",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "amt_flour",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "amt_tomatoes",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "amt_cheese",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "aa25e147-f030-4c09-858e-5ba7efd20a4a",
       "rows": [
        [
         "revenue",
         "1752348.4078824243",
         "-3671.6555642109524",
         "82.02105899969825",
         "73.33079976049092",
         "384.5005354863867"
        ],
        [
         "total_customers",
         "-3671.6555642109524",
         "7342.909472185136",
         "3.612878344589165",
         "-22.544256957087928",
         "-6.924364070540847"
        ],
        [
         "amt_flour",
         "82.02105899969825",
         "3.612878344589165",
         "164.93397445921298",
         "-6.928481092691553",
         "1.9176056462812185"
        ],
        [
         "amt_tomatoes",
         "73.33079976049092",
         "-22.544256957087928",
         "-6.928481092691553",
         "62.209302900386234",
         "-0.48715455227517007"
        ],
        [
         "amt_cheese",
         "384.5005354863867",
         "-6.924364070540847",
         "1.9176056462812185",
         "-0.48715455227517007",
         "26.211397934710895"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue</th>\n",
       "      <th>total_customers</th>\n",
       "      <th>amt_flour</th>\n",
       "      <th>amt_tomatoes</th>\n",
       "      <th>amt_cheese</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>revenue</th>\n",
       "      <td>1.752348e+06</td>\n",
       "      <td>-3671.655564</td>\n",
       "      <td>82.021059</td>\n",
       "      <td>73.330800</td>\n",
       "      <td>384.500535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_customers</th>\n",
       "      <td>-3.671656e+03</td>\n",
       "      <td>7342.909472</td>\n",
       "      <td>3.612878</td>\n",
       "      <td>-22.544257</td>\n",
       "      <td>-6.924364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amt_flour</th>\n",
       "      <td>8.202106e+01</td>\n",
       "      <td>3.612878</td>\n",
       "      <td>164.933974</td>\n",
       "      <td>-6.928481</td>\n",
       "      <td>1.917606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amt_tomatoes</th>\n",
       "      <td>7.333080e+01</td>\n",
       "      <td>-22.544257</td>\n",
       "      <td>-6.928481</td>\n",
       "      <td>62.209303</td>\n",
       "      <td>-0.487155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amt_cheese</th>\n",
       "      <td>3.845005e+02</td>\n",
       "      <td>-6.924364</td>\n",
       "      <td>1.917606</td>\n",
       "      <td>-0.487155</td>\n",
       "      <td>26.211398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      revenue  total_customers   amt_flour  amt_tomatoes  \\\n",
       "revenue          1.752348e+06     -3671.655564   82.021059     73.330800   \n",
       "total_customers -3.671656e+03      7342.909472    3.612878    -22.544257   \n",
       "amt_flour        8.202106e+01         3.612878  164.933974     -6.928481   \n",
       "amt_tomatoes     7.333080e+01       -22.544257   -6.928481     62.209303   \n",
       "amt_cheese       3.845005e+02        -6.924364    1.917606     -0.487155   \n",
       "\n",
       "                 amt_cheese  \n",
       "revenue          384.500535  \n",
       "total_customers   -6.924364  \n",
       "amt_flour          1.917606  \n",
       "amt_tomatoes      -0.487155  \n",
       "amt_cheese        26.211398  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One important point to note is that along the primary diagonal (from top-left to bottom-right), we see the same variance values that we calculated for each individual column earlier on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization, Eigenvalues, and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We now have a matrix of variance values for our features. \n",
    "- The next step in PCA revolves around *matrix factorization*. \n",
    "- Without going into too much detail, our goal with matrix factorization is to find a pair of smaller matrices whose product would equal our covariance matrix. \n",
    "- Another way of thinking about it: \n",
    "    - we want to find a smaller matrix that captures the majority of our information.\n",
    "\n",
    "<br>\n",
    "\n",
    "- An important part of this matrix factorization are *Eigenvectors*. \n",
    "- Eigenvectors are vectors (mathematical concepts that have direction and magnitude) that do not change direction when a transformation is applied to them.\n",
    "- In the context of data matrices, these eigenvectors give us a direction to “rotate” the dataset in n-dimensional space so we can look at the entire dataset from a simplified perspective.\n",
    "- The *eigenvalues* are related to the relative variation described by each principal component.\n",
    "\n",
    "<br>\n",
    "\n",
    "- For a matrix `A`, the eigenvectors and eigenvalues are the solutions to the following equation:\n",
    "$$ det(A - \\lambda I) $$\n",
    "- After some linear algebra, for our covariance matrix, we are looking for the solution to the following matrix, which will be our eigenvectors and eigenvalues.\n",
    "$$ det \\begin{bmatrix} 1752348.4078824243 - \\lambda & -3671.6555642109524 & 82.02105899969825 & 73.33079976049092 & 384.5005354863867 \\\\ -3671.6555642109524 & 7342.909472185136 - \\lambda & 3.612878344589165 & -22.544256957087928 & -6.924364070540847 \\\\ 82.02105899969825 & 3.612878344589165 & 164.93397445921298 - \\lambda & -6.928481092691553 & 1.9176056462812185 \\\\ 73.33079976049092 & -22.544256957087928 & -6.928481092691553 & 62.209302900386234 - \\lambda & -0.48715455227517007 \\\\ 384.5005354863867 & -6.924364070540847 & 1.9176056462812185 & -0.48715455227517007 & 26.211397934710895 - \\lambda \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- All of the underlying math behind PCA results in principal components, but what exactly are they? \n",
    "- Principal components are a linear combination of all the input features from the original dataset. \n",
    "- By using the eigenvectors we calculated earlier, we can “rotate” our dataset features from an n-dimensional space into a 2-dimensional space, which is easier for us to understand and analyze.\n",
    "- To illustrate this point, let’s return to our pizza dataset. We can observe the correlation between our `revenue` and `total_customers` features.\n",
    "\n",
    "```python\n",
    "sns.scatterplot(x='total_customers', y='revenue', data=df)\n",
    "```\t\n",
    "\n",
    "<img src=\"Images/revenue-customer-scatterplot.webp\" width=\"700\">\n",
    "\n",
    "- We can see a positive correlation between these two features, and could use that information to guide any analysis we perform. \n",
    "- We can also do correlation plots for every combination of features, like so:\n",
    "\n",
    "```python\n",
    "sns.pairplot(df)\n",
    "```\n",
    "\n",
    "<img src=\"Images/all-scatterplots.webp\" width=\"800\">\n",
    "\n",
    "- Each individual combination of features will have its own correlation and variance, both of which provide valuable information about that relationship. \n",
    "- When comparing two features at a time, these relationships are more understandable.\n",
    "- If we wanted to, however, look at all of the feature relationships and information at once, it would be very difficult to decipher, as we cannot visualize data in a 5-dimensional space.\n",
    "- By using PCA, however, we can reduce the dimensionality of our dataset into a 2-dimensional dataset, allowing for better visualization. \n",
    "- Let’s see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-540.63114048,   53.00605385],\n",
       "       [2052.28185925,   -5.5292275 ],\n",
       "       [1068.18083613,  -74.57687627],\n",
       "       ...,\n",
       "       [ 219.74177294,  141.55706742],\n",
       "       [1853.0866606 ,   45.00856271],\n",
       "       [ 583.34077584, -103.63115492]], shape=(750, 2))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_array = pca.fit_transform(df)\n",
    "pca_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we can see, by running PCA on our original dataset, we were able to take our 5 features and reduce the dimensions down to 2 principal components. \n",
    "- With 2 dimensions, we can now plot the data on a single scatterplot:\n",
    "\n",
    "```python\t\n",
    "sns.scatterplot(pca_array[:,0], pca_array[:,1])\n",
    "```\n",
    "\n",
    "<img src=\"Images/pca_pizza_plot.webp\" width=\"700\">\n",
    "\n",
    "- While it can be difficult to interpret what this new data matrix is showing us, it does hold valuable information that can be used in a variety of contexts. \n",
    "- The axes of this chart are the two most impactful principal components as part of our analysis, and were the two that we decided to keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given dataset, we start by calculating a `covariance matrix` for all of our features. Afterwards, we perform `matrix factorization`, which will separate out the dataset and give us two results:\n",
    "1) `eigenvectors`, also known as Principal Components, which define the direction, or \"rotation\", of our new data space\n",
    "2) `eigenvalues`, which determine the magnitude of that new data space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The How, Where, and Why of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA serves an important role in many different parts of data science and analytics in general, as this process allows us to maximize the amount of information we can extract from data while reducing computational time down the line. \n",
    "- We just saw a common use case for PCA with our pizza dataset. \n",
    "- We took a higher dimensional dataset (5 dimensions in our case), and reduced it down to 2 dimensions. \n",
    "- This two-dimensional dataset can now be an input to a variety of Machine Learning models. \n",
    "- For example, we could use this new dataset as part of a forecasting model, or perform linear regression. \n",
    "- These techniques would have been much more difficult prior to performing PCA.\n",
    "\n",
    "<br>\n",
    "\n",
    "- PCA is also inherently an unsupervised learning algorithm and can be used to identify clusters in data on its own. \n",
    "- Very similar to the popular k-means algorithms, PCA will look at overall similarities between the different features in a dataset. \n",
    "- When we set the number of principal components to keep, we are defining the number of similar “rotations” of our dataset, which will act very much like a cluster of their own. \n",
    "- Typically, many practitioners will implement PCA as a precursor to other clustering algorithms to augment the accuracy, but it is an interesting application to do clustering with PCA alone!\n",
    "\n",
    "<br>\n",
    "\n",
    "- Another, very powerful, application of PCA is with image processing. \n",
    "- Images hold a vast amount of information in each file, and analyzing this information can have very useful applications. \n",
    "- Image classification, for example, uses algorithms to detect the subject of an image, or find a particular object within the image. \n",
    "- Overall, it can be very costly to process image data, due to the high dimensionality it has. \n",
    "- By applying PCA, however, practitioners can reduce the number of features for the image with minimal information loss and continue their processing.\n",
    "\n",
    "<br>\n",
    "\n",
    "- Below are two images from a Fruit and Vegetable Image dataset. \n",
    "- The first image is the original image, and the second is the reconstructed image using the calculated eigenvectors after performing PCA. \n",
    "- Note that, in this very quick example, there is minimal information loss, meaning that data could be effectively used for analytics.\n",
    "\n",
    "<img src=\"Images/original_pineapple.webp\" width=\"400\">\n",
    "<img src=\"Images/reconstructed_pineapple.webp\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this article, you learned the underlying mathematics behind Principal Component Analysis (PCA) and got a bird’s eye view of how, when, and where to implement PCA. \n",
    "- Principal Component Analysis is a powerful tool that provides a mechanism to reduce dimensionality and simplify datasets without losing the valuable information they inherently contain. \n",
    "- The following image summarizes the different applications of PCA and we are now ready to delve into these in detail!\n",
    "\n",
    "<img src=\"Images/PCA Summary.webp\" width=\"800\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy-XphA9WxU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
